# ğŸ Deep Reinforcement Learning Snake Agent ğŸ¤–ğŸ”¥

A fully custom **Snake Environment** + **Deep Q-Learning Agent (DQN)** built from scratch using:

- ğŸ§  TensorFlow / Keras  
- ğŸ”¢ NumPy  
- ğŸ“Š Matplotlib  
- ğŸ® Custom Grid-Based Snake Environment  
- â™»ï¸ Experience Replay  
- ğŸ¯ Epsilon-Greedy & Boltzmann Policies  
- ğŸ’¾ Model Save / Load Support  

---

# ğŸš€ Project Overview

This project implements a **Deep Q-Network (DQN)** agent that learns to play Snake autonomously using reinforcement learning.

The agent:

- Observes a local grid around its head ğŸ‘€  
- Converts the observation into a feature vector  
- Predicts Q-values using a neural network ğŸ§   
- Learns from replay memory ğŸ“š  
- Gradually shifts from exploration to exploitation âš–ï¸  
- Optimizes cumulative reward over episodes ğŸ“ˆ  

---

# ğŸ—ï¸ Architecture

## ğŸ Environment (`SnakeEnv`)

A fully custom 2D grid environment:

- Random food spawning ğŸ  
- Snake growth mechanics  
- Collision detection (wall & body) ğŸš§  
- Reward shaping using distance-to-food ğŸ“  
- Configurable grid size  

### ğŸ¯ Reward System

| Event        | Reward |
|-------------|--------|
| ğŸ Food      | +12    |
| ğŸ“ Closer    | +1.5   |
| ğŸ§± Snake Hit | -11    |
| âŒ Out       | -10    |
| ğŸ”„ Reverse   | -1     |

Reward shaping accelerates convergence and stabilizes training.

---

## ğŸ¤– Agent (`SnakeAgent`)

### ğŸ”¥ Core Features

- Deep Q-Network (Fully Connected)
- Experience Replay Buffer
- Batch Training
- Gamma Discounting
- Epsilon Decay
- Temperature Scheduling
- Model Auto-Saving
- SMA (Simple Moving Average) smoothing
- Training & Testing modes

---

# ğŸ§  Model Architecture Example

```python
Agent.CreateModel(nDense=[1024], Activation='leaky_relu')
```

### Network Structure

```
Input Layer (State Vector)
        â†“
Dense(1024, leaky_relu)
        â†“
Dense(4, linear)  â† Q-values
```

---

# âš™ï¸ Training Configuration Example

```python
Agent = ag.SnakeAgent(
    Env,
    2,
    2,
    nEpisode=400,
    mStep=150,
    sMemory=1024,
    sBatch=64,
    Temperature1=10,
    Temperature2=1.5,
    TrainOn=32
)
```

---

# ğŸ“Š Exploration Scheduling

### ğŸ¯ Epsilon Decay
- Starts at `0.99`
- Ends at `0.01`
- Linear decay across episodes

### ğŸŒ¡ Temperature Decay
- Logarithmic decay
- Smooth transition from exploration to exploitation

---

# ğŸ“ˆ Training Visualizations

The following plots demonstrate the learning progression of the agent.

---

## 1ï¸âƒ£ Agent Action Reward (â‰ˆ 400 Actions)

This plot shows **reward per action** during early training.

ğŸ“Œ Characteristics:

- High variance  
- Random exploration dominates  
- Unstable Q-value estimates  

```markdown
![Agent Action Reward 400](Figure_2.png)
```

---

## 2ï¸âƒ£ Agent Episode Reward (â‰ˆ 35,000 Actions)

This plot shows **total reward per episode** after extended training.

ğŸ“Œ Characteristics:

- Reward stabilization  
- Upward SMA trend  
- Improved food-seeking behavior  
- Reduced collision frequency  

```markdown
![Agent Episode Reward 35000](Figure_1.png)
```

---

# ğŸ§ª Generate Plots

```python
Agent.PlotActionLog(400)
Agent.PlotEpisodeLog(10)
```

---

# ğŸ§© Available Policies

| Policy | Description |
|--------|------------|
| `'R'`  | Random |
| `'G'`  | Greedy |
| `'EG'` | Epsilon-Greedy |
| `'B'`  | Boltzmann (Softmax) |

---

# ğŸ Results Summary

After ~35,000 actions:

âœ… Agent learns directional food-seeking  
âœ… Collision rate decreases significantly  
âœ… Reward curve trends upward  
âœ… Exploration anneals properly  
âœ… Q-values stabilize  

---

# ğŸ’¡ Future Improvements

- ğŸ§  Target Network (Double DQN)  
- ğŸ“¦ Prioritized Experience Replay  
- ğŸ CNN-based state encoding  
- ğŸ“Š TensorBoard logging  
- ğŸ® Larger grid experiments  
- ğŸš€ GPU optimization  

---

# â­ Why This Project Is Awesome

- Pure Reinforcement Learning from scratch ğŸ”¥  
- Custom-built environment (no Gym) ğŸ§©  
- Fully configurable hyperparameters âš™ï¸  
- Clean modular implementation ğŸ§¼  
- Clear training visualization ğŸ“Š  

---

# ğŸ Deep RL Snake â€” From Random to Intelligent

> Watching the reward curve stabilize after 35K actions is pure dopamine for ML engineers ğŸ¤¯ğŸ“ˆ

---

If you like this project:

â­ Star it  
ğŸ´ Fork it  
ğŸ§  Improve it  
ğŸš€ Train it longer  

Happy Reinforcement Learning! ğŸ¯ğŸ”¥